{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Text Analysis and Retrieval System\n",
        "## Introduction\n",
        "This notebook demonstrates the creation of an advanced text analysis and retrieval system leveraging a multitude of powerful libraries and APIs, including `langchain`, `pymupdf`, `cohere`, `pinecone-client`, `PyPDF2`, `openai`, `datasets`, and `ragas`. It highlights the integration and use of external API services from Cohere, Pinecone, and OpenAI for embedding, indexing, and model inference, illustrating the cutting-edge capabilities in processing and analyzing textual data.\n",
        "## Main Functionality Overview\n",
        "- **Environment Setup:** Initial setup involves installing necessary libraries and configuring API keys for Cohere, Pinecone, and OpenAI services, ensuring the seamless integration of these tools for our text analysis tasks.\n",
        "- **Vector Database Integration:** A serverless index is created with Pinecone, a vector database designed for efficient large-scale vector search operations, facilitating rapid text retrieval.\n",
        "- **Embedding Generation and PDF Processing:** The notebook utilizes `CohereEmbeddings` for generating text embeddings and `PdfReader` for the extraction and processing of PDF documents. This includes support for multiple languages, showcasing the system's versatility.\n",
        "- **Data Indexing and Retrieval:** Text data is processed by splitting into manageable chunks, embedding these chunks, and then indexing them in Pinecone. This allows for fast and efficient retrieval of information, demonstrating the practical application of vector databases in handling and searching large datasets.\n",
        "This structured approach enables the notebook to serve as a comprehensive guide to building an advanced text analysis and retrieval system, incorporating the latest technologies in natural language processing and database management."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Install and import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3> This section covers the installation and importing of necessary libraries for the project. </br>\n",
        "It ensures all required tools are available for the analysis and retrieval tasks ahead. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install langchain\n",
        "# pip install pymupdf\n",
        "# pip install cohere\n",
        "# pip install pinecone-client\n",
        "# pip install PyPDF2\n",
        "# pip install openai\n",
        "# pip install datasets\n",
        "# pip install ragas\n",
        "# pip install --upgrade --quiet  langchain-google-genai pillow\n",
        "# pip install python-dotenv\n",
        "\n",
        "import os\n",
        "import random\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.vectorstores import Pinecone as PineconeStore\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from datasets import Dataset\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from dotenv import load_dotenv\n",
        "from ragas import evaluate\n",
        "from langchain_openai.chat_models import AzureChatOpenAI\n",
        "import google.generativeai as genai\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Create pinecode index and load pdf <h1>\n",
        "\n",
        "<h3> Here, we initialize a Pinecone index for efficient vector search and load a PDF document for analysis. </br>\n",
        "This step is crucial for setting up our data storage and retrieval system. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "h3X5dIsU6FZn"
      },
      "outputs": [],
      "source": [
        "load_dotenv('keys.env')\n",
        "\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
        "COHERE_API_KEY = os.getenv('COHERE_API_KEY')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "INDEX_NAME = \"quickstart\"\n",
        "\n",
        "# Create a serverless index\n",
        "# \"dimension\" needs to match the dimensions of the vectors you upsert\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "embeddings = CohereEmbeddings(model = \"embed-multilingual-v3.0\", cohere_api_key=COHERE_API_KEY)\n",
        "\n",
        "pc.delete_index(INDEX_NAME)\n",
        "\n",
        "####\n",
        "if INDEX_NAME not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(name=INDEX_NAME, dimension=1024,\n",
        "    spec=ServerlessSpec(cloud='aws', region='us-west-2')\n",
        "    )\n",
        "    # Load PDF with hebrew support\n",
        "    pdf_file = open('example.pdf', 'rb')  # Open your PDF in binary mode\n",
        "    reader = PdfReader(pdf_file)  # Create a PdfFileReader object\n",
        "    heb_pages = reader.pages\n",
        "\n",
        "    pages = \"\"\n",
        "\n",
        "    # Create large string\n",
        "    for page in reader.pages:\n",
        "      pages += page.extract_text()\n",
        "\n",
        "    # Split the PDF into smaller chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "    texts = text_splitter.split_text(pages)\n",
        "    text_content = [doc for doc in texts]\n",
        "\n",
        "    docsearch = PineconeStore.from_texts(text_content, embeddings, index_name=INDEX_NAME)\n",
        "else:\n",
        "  text_field = \"text\"\n",
        "\n",
        "  # switch back to normal index for langchain\n",
        "  index = pc.Index(INDEX_NAME)\n",
        "\n",
        "  docsearch = PineconeStore(\n",
        "      index, embeddings, text_field\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Create large language model using azure </h1> \n",
        "<h3> In this part, we leverage Azure's capabilities to create a large language model. </br>\n",
        "This model will play a key role in processing and understanding the textual content from our documents. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "GPT_DEPLOYMENT_NAME=\"chatgpt_16k\"\n",
        "\n",
        "AZURE_OPENAI_API_KEY = os.getenv('AZURE_OPENAI_API_KEY')\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
        "\n",
        "\n",
        "llm = AzureChatOpenAI(\n",
        "    openai_api_version=\"2023-05-15\",\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    azure_deployment=GPT_DEPLOYMENT_NAME,\n",
        "    model='azure',\n",
        "    validate_base_url=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Retreive k random documents from index. </br>\n",
        "<h3>For each document retrieved, we generate a question whose answer lies within the document's content, storing these questions in a list named `questions`. </br>\n",
        "This exercise aims to test the retrieval accuracy and relevance of the indexed data. \n",
        "\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nTYWD9MLpop3"
      },
      "outputs": [],
      "source": [
        "retriever = docsearch.as_retriever()\n",
        "random_documents = random.choices(texts, k=2)\n",
        "\n",
        "questions = []\n",
        "documents = []\n",
        "\n",
        "# Create k questions who's answer is in k docs respectively\n",
        "for doc in random_documents:\n",
        "  template = \"Generate a question in hebrew who's answer is within the following text: {doc}\"\n",
        "  prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "  # Setup RAG pipeline\n",
        "  rag_chain = (\n",
        "      {\"context\": retriever,  \"doc\": RunnablePassthrough()}\n",
        "      | prompt\n",
        "      | llm\n",
        "      | StrOutputParser()\n",
        "  )\n",
        "\n",
        "  questions.append(rag_chain.invoke(doc))\n",
        "  documents.append(doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Create a chat prompt using Gemini. </h1>\n",
        "<h3>Using the document as context, we send prompts requesting answers to each question, respectively, through Gemini. \n",
        "The responses are saved in a list called `answers`, showcasing the interaction with the model and its understanding of the context. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "_EIJ4aAcEBUx",
        "outputId": "cf455e59-5754-4633-8f41-94298a4f59e3"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-pro\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "generation_config = {\n",
        "    \"temperature\": 1,\n",
        "    \"top_p\": 1,\n",
        "    \"top_k\": 1,\n",
        "    \"max_output_tokens\": 1024,\n",
        "}\n",
        "safety_settings = [\n",
        "    {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "    {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "    {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "    {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n",
        "]\n",
        "\n",
        "gemini = genai.GenerativeModel(\n",
        "    model_name=model,\n",
        "    safety_settings=safety_settings,\n",
        "    generation_config=generation_config,\n",
        ")\n",
        "\n",
        "answers = []\n",
        "chat=gemini.start_chat()\n",
        "\n",
        "for i in range(len(documents)):\n",
        "  question = questions[i]\n",
        "  document = documents[i]\n",
        "\n",
        "  # Define prompt template\n",
        "  template = f\"\"\"You are an assistant for question-answering tasks.\n",
        "  Use the following pieces of retrieved context and question below to answer the question in hebrew.\n",
        "  Use two sentences maximum and keep the answer concise.\n",
        "  Question: {question}\n",
        "  Context: {document}\n",
        "  Answer:\n",
        "  \"\"\"\n",
        "\n",
        "  response = chat.send_message(template)\n",
        "  answers.append(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1>Prepare a dataset to be used with metrics evalution.</h1>\n",
        "<h3> This section involves preparing a dataset that will be used to evaluate the effectiveness of our retrieval and question-answering system. <br>\n",
        "It sets the stage for assessing performance and accuracy. </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "38s2ncficek5",
        "outputId": "89df61cb-6ef5-4862-a943-a57b17b0c1b3"
      },
      "outputs": [],
      "source": [
        "ground_truths = []\n",
        "\n",
        "for answer in answers:\n",
        "    list = []\n",
        "    list.append(answer)\n",
        "    ground_truths.append(list)\n",
        "\n",
        "print(len(questions))\n",
        "print(len(answers))\n",
        "\n",
        "contexts = []\n",
        "answersb = []\n",
        "\n",
        "# Inference\n",
        "for query in questions:\n",
        "    contexts.append([docs.page_content for docs in retriever.get_relevant_documents(query)])\n",
        "    response = chat.send_message(query)\n",
        "    answersb.append(response.text)\n",
        "\n",
        "# To dict\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answersb,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truths\": ground_truths\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h1> Evaluate metrics using dataset  </h1>\n",
        "<h3> Finally, we evaluate various metrics using the prepared dataset. </br>\n",
        "This evaluation will help us understand the strengths and weaknesses of our system, providing insights into areas for improvement.</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6137575dbc594380a07e666033e19236",
            "34e7d3d0e1ae4cabac32019b352717b6",
            "049f7dce22864d12ae765c493e544ccf",
            "61adf235bb8a4539b2a00d102b2bafab",
            "107a4269f75f4c929a7ccfd79848fba0",
            "77ebb1a9626b4c07a09e193630a24581",
            "80fe6f4c7f2a41168e611cbc49613ee1",
            "31c50cd0be7b48f5b609046dc8921583",
            "971d9a403eb540be9bd73d0109c7ed30",
            "bddbcf2e1a214c838398685ece0a006e",
            "c95cdfa709e147efbceb6c7ad3e90737"
          ]
        },
        "id": "OZDifzlk9TfN",
        "outputId": "67b5163c-860b-4744-c5e8-e4691cc73aaa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "result = evaluate(\n",
        "    llm =llm,\n",
        "    dataset = dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        answer_relevancy,\n",
        "        faithfulness\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()\n",
        "display(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "049f7dce22864d12ae765c493e544ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31c50cd0be7b48f5b609046dc8921583",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_971d9a403eb540be9bd73d0109c7ed30",
            "value": 0
          }
        },
        "107a4269f75f4c929a7ccfd79848fba0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c50cd0be7b48f5b609046dc8921583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e7d3d0e1ae4cabac32019b352717b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77ebb1a9626b4c07a09e193630a24581",
            "placeholder": "​",
            "style": "IPY_MODEL_80fe6f4c7f2a41168e611cbc49613ee1",
            "value": "Evaluating:   0%"
          }
        },
        "6137575dbc594380a07e666033e19236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34e7d3d0e1ae4cabac32019b352717b6",
              "IPY_MODEL_049f7dce22864d12ae765c493e544ccf",
              "IPY_MODEL_61adf235bb8a4539b2a00d102b2bafab"
            ],
            "layout": "IPY_MODEL_107a4269f75f4c929a7ccfd79848fba0"
          }
        },
        "61adf235bb8a4539b2a00d102b2bafab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bddbcf2e1a214c838398685ece0a006e",
            "placeholder": "​",
            "style": "IPY_MODEL_c95cdfa709e147efbceb6c7ad3e90737",
            "value": " 0/15 [05:42&lt;?, ?it/s]"
          }
        },
        "77ebb1a9626b4c07a09e193630a24581": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80fe6f4c7f2a41168e611cbc49613ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "971d9a403eb540be9bd73d0109c7ed30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bddbcf2e1a214c838398685ece0a006e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c95cdfa709e147efbceb6c7ad3e90737": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
